{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRADIENT TAPE BASICS",
      "provenance": [],
      "authorship_tag": "ABX9TyNZcSVHdCv125oNC2qcPos1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobbyorr007/GRADIENT-TAPE-BASICS/blob/main/GRADIENT_TAPE_BASICS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ulsYLbTLrg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTS**"
      ],
      "metadata": {
        "id": "p3KqCxt7Luhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "HFe4sM3hL6w-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE ON THE BASICS OF GRADIENT TAPE**\n",
        "\n",
        "\n",
        "Using tf.GradientTape() for automatic differentiations."
      ],
      "metadata": {
        "id": "ErtAbFvAMJhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a 2x2 array of ones\n",
        "x = tf.ones((2,2))\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  #Record the action performed on tensor x with watch\n",
        "  t.watch(x)\n",
        "\n",
        "  #Define y as the sum of elements in x\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "  #Let z be the square of y\n",
        "  z = tf.square(y)\n",
        "\n",
        "#Now we get the differentiation of z with respect to the original tensor x\n",
        "dz_dx = t.gradient(z,x)\n",
        "\n",
        "# Print out the result\n",
        "dz_dx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCBhbICUL_vb",
        "outputId": "f3f3d648-f017-4a12-c501-a2518ec20640"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[8., 8.],\n",
              "       [8., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BY DEFAULT GRADIENT TAPE EXPIRES AFTER ONE USE**\n",
        "\n",
        "Gradient Tape is set to (persistence=False) by default, this makes it expires after one usage of calculating differentiation.\n",
        "if you want to use Gradient Tape for multiple gradient calculation, it should be set to (persistence=True)."
      ],
      "metadata": {
        "id": "RssWdF0WQMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.ones((2,2))\n",
        "\n",
        "# Set (persistence = True) so that the Tape can be reused.\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x)\n",
        "\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "  z = tf.square(y)\n",
        "\n",
        "# Compute dz_dx.\n",
        "dz_dx = t.gradient(z,x)\n",
        "dz_dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB0vuSnwQP-W",
        "outputId": "b8481c0f-4896-4d6c-b632-1456397719a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[8., 8.],\n",
              "       [8., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that  I set persistence=True, we can reuse the Tape**\n",
        "\n",
        "\n",
        "Lets try computing another gradient"
      ],
      "metadata": {
        "id": "9fgoxX0LSxdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx = t.gradient(y,x)\n",
        "dy_dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGJGtBE8OaO0",
        "outputId": "529c938c-4c09-4867-f60a-f524a4b53a07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[1., 1.],\n",
              "       [1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To stop the Tape when you don't need it again, i.e when you are done computing gradient we use this command;"
      ],
      "metadata": {
        "id": "Pbn9DkBJTYxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the reference to t\n",
        "del t"
      ],
      "metadata": {
        "id": "9I0ORPkSTQu0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NESTED GRADIENT TAPES**\n",
        "\n",
        "I computted a higher order derivaties by nesting the Gradient Tape:\n",
        "\n",
        "***These are the acceptable indentation of the first gradient calculation***\n",
        "\n",
        "\n",
        "The first gradient calculation should at least be inside the outer with block."
      ],
      "metadata": {
        "id": "0zXaJe2anKly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.0)\n",
        "with tf.GradientTape() as tape_2:\n",
        "  with tf.GradientTape() as tape_1:\n",
        "    y = x * x * x\n",
        "\n",
        "  # The first gradient calculation should at least occur within the outer with block.\n",
        "  dy_dx = tape_1.gradient(y,x)\n",
        "d2y_d2x = tape_2.gradient(dy_dx, x)\n",
        "\n",
        "print(dy_dx)\n",
        "print(d2y_d2x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwU6JzYZTtx3",
        "outputId": "437cd739-b909-45ce-fdbb-f14988a0134f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(27.0, shape=(), dtype=float32)\n",
            "tf.Tensor(18.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Where not to indent the first gradient calculation**\n",
        "\n",
        "If the first gradient calculation is indented outside the outer with block, it would work but wont persist for the second gradient calculation. Even if persistence is set to true, this wouldnt work.\n",
        "\n",
        "\n",
        "**Where to indent the second gradient calculation**\n",
        "\n",
        "The second gradient calcultion should be indented at the same indentation or  tab to the left from the first gradient calculation or more.\n",
        "\n",
        "It shouldnt be indented more than the first gradient calculation.\n",
        "\n",
        "This is acceptable;\n",
        "\n",
        "  dy_dx = tape_1.gradient(y,x)\n",
        "\n",
        "  d2y_d2x = tape_2.gradient(dy_dx, x)\n",
        "\n",
        " **OR**\n",
        "\n",
        "      dy_dx = tape_1.gradient(y,x)\n",
        "\n",
        "d2y_d2x = tape_2.gradient(dy_dx, x)\n",
        "\n",
        "**NOT**\n",
        "\n",
        "  dy_dx = tape_1.gradient(y,x)\n",
        "\n",
        "      d2y_d2x = tape_2.gradient(dy_dx, x)"
      ],
      "metadata": {
        "id": "jtihSzEYc6oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yZa1GslwcM95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}